{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Image classification with Vision Transformer (cifar10)\n",
    "\n",
    "**Original Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Modified by Team 4**<br>\n",
    "**Last Modified:** Aug, 08, 2022 <br>\n",
    "**Description:** Implementing the Vision Transformer (ViT) model for image classification. <br>\n",
    "The original version uses cifar100 as the dataset. Now it uses cifar10 as a comparsion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "model by Alexey Dosovitskiy et al. for image classification,\n",
    "and demonstrates it on the CIFAR-100 dataset.\n",
    "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
    "image patches, without using convolution layers.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
    "which can be installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "image_size = 32  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 18:35:49.910384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd6849b8a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd6849b8a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 32 X 32\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 25\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Patches.call of <__main__.Patches object at 0x7fd661c18cc0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Patches.call of <__main__.Patches object at 0x7fd661c18cc0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PatchEncoder.call of <__main__.PatchEncoder object at 0x7fd661d37898>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PatchEncoder.call of <__main__.PatchEncoder object at 0x7fd661d37898>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd657d55e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd657d55e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "176/176 [==============================] - ETA: 0s - loss: 1.9617 - accuracy: 0.3266 - top-5-accuracy: 0.8226WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd6500b0ae8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd6500b0ae8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "176/176 [==============================] - 169s 913ms/step - loss: 1.9617 - accuracy: 0.3266 - top-5-accuracy: 0.8226 - val_loss: 1.4893 - val_accuracy: 0.4560 - val_top-5-accuracy: 0.9196\n",
      "Epoch 2/20\n",
      " 16/176 [=>............................] - ETA: 2:20 - loss: 1.6569 - accuracy: 0.4019 - top-5-accuracy: 0.8818"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
    "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
    "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
    "\n",
    "Note that the state of the art results reported in the\n",
    "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
    "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
    "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
    "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
    "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
    "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
    "In practice, it's recommended to fine-tune a ViT model\n",
    "that was pre-trained using a large, high-resolution dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_with_vision_transformer",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
